{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MAML-PyTorch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqzCA3JCOqzrI8wFKNT+j6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshStrong/MAML/blob/master/MAML_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q3wlQVeC4B5",
        "colab_type": "text"
      },
      "source": [
        "# Model Agnostic Meta Learning (MAML)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysxQDVMTDGAW",
        "colab_type": "text"
      },
      "source": [
        "Make use of google colabs GPU for faster training via PyTorch. \\\\\n",
        "Original MAML paper: https://arxiv.org/pdf/1703.03400.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLX8j6wtCxBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih8cWtXJv4Ca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9173fb63-cb46-4f24-9e86-8745505f35d6"
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgI4DX7f8l4H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "57d60558-cb1b-4f43-8bbe-e34263c5c573"
      },
      "source": [
        "!pip install torchviz\n",
        "from graphviz import Digraph\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "# make_dot was moved to https://github.com/szagoruyko/pytorchviz\n",
        "from torchviz import make_dot"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.5.1+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmkawvM0xPDl",
        "colab_type": "text"
      },
      "source": [
        "# Implementing the original regression tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77mDMCzkDYdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sine_Task():\n",
        "  def __init__(self, amplitude, phase, xmin, xmax):\n",
        "    self.amplitude = amplitude\n",
        "    self.phase = phase\n",
        "    self.xmin = xmin\n",
        "    self.xmax = xmax\n",
        "\n",
        "  def oracle(self, x):\n",
        "    \"\"\"\n",
        "    Oracle: returns output of sin function with given amplitude, phase and input x\n",
        "\n",
        "    PARAMETERS:\n",
        "    1. x - input\n",
        "    \"\"\"\n",
        "    return self.amplitude * np.sin(self.phase + x)\n",
        "\n",
        "  def sample_data(self, size=1):\n",
        "    \"\"\"\n",
        "      sample_data: sample input/output of given instance of Sine_Task with set variables\n",
        "\n",
        "      PARAMETERS:\n",
        "      1. size - amount of sampled data\n",
        "    \"\"\"\n",
        "    x = torch.rand(size)*(self.xmax-self.xmin) - self.xmax\n",
        "    y = self.oracle(x)\n",
        "    x = x.unsqueeze(1).cuda()\n",
        "    y = y.unsqueeze(1).cuda()\n",
        "\n",
        "    return x, y"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MWPJLo_Gkmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sine_Task_Distribution():\n",
        "  def __init__(self, amplitude_min, amplitude_max, phase_min, phase_max, xmin, xmax):\n",
        "    self.amplitude_min = amplitude_min\n",
        "    self.amplitude_max = amplitude_max\n",
        "    self.phase_min = phase_min\n",
        "    self.phase_max = phase_max\n",
        "    self.xmin = xmin\n",
        "    self.xmax = xmax\n",
        "\n",
        "  def sample_task(self):\n",
        "    # Sample a random amplitude from the range []\n",
        "    amplitude = np.random.uniform(self.amplitude_min, self.amplitude_max)\n",
        "    phase = np.random.uniform(self.phase_min, self.phase_max)\n",
        "\n",
        "    return Sine_Task(amplitude, phase, self.xmin, self.xmax)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F59HhGTgIvG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Regressor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Regressor, self).__init__()\n",
        "    self.fc1 = nn.Linear(1, 40)\n",
        "    self.fc2 = nn.Linear(40, 40)\n",
        "    self.fc3 = nn.Linear(40, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvUZdnfOwG0b",
        "colab_type": "text"
      },
      "source": [
        "The (shortened) MAML algorithm:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Sample task (or a batch of tasks) $\\mathcal{T}_i$.\n",
        "*   Sample $D_i^{tr}, D_i^{test}$ from sampled task $\\mathcal{T}_i$.\n",
        "*   Inner Loop: Optimise meta-parameters $\\theta$ on task $D_i$ to produce task-specific optimal parameters $\\phi_i$: $\\phi_i \\leftarrow \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\theta, D_i^{tr})$\n",
        "*   Outer Loop: Update $\\theta$ using stochastic gradient descent:\n",
        "$\\theta \\leftarrow \\theta - \\beta \\nabla_\\theta \\mathcal{L}(\\phi, D_i^{test})$\n",
        "\n",
        "where $\\mathcal{L}(\\cdot, \\cdot)$ is the chosen loss function of the network.\n",
        "\n",
        "\n",
        "\n",
        "Let $U(\\theta, D^{tr}) := \\phi = \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\theta, D^{tr})$ denote the update rule used for optimising $\\phi$.\n",
        "\n",
        "\n",
        "\n",
        "The meta-optimisation objective is given as \n",
        "\\begin{align*}\n",
        "    \\underset{\\theta}{\\min}\\,\\,\\mathcal{L}(\\phi, D^{test}) = \\underset{\\theta}{\\min}\\,\\,\\mathcal{L}(U(\\theta, D^{tr}), D^{test}).\n",
        "\\end{align*}\n",
        "We require $\\frac{d}{d\\theta}\\mathcal{L}(\\phi, D^{test})$\n",
        "\\begin{align*}\n",
        "    \\frac{d}{d\\theta}\\mathcal{L}(\\phi, D^{test}) &= \\frac{d}{d\\theta}\\mathcal{L}(U(\\theta, D^{tr}), D^{test})\\\\\n",
        "    &= \\underbrace{\\nabla_{\\Theta}\\mathcal{L}(\\Theta, D^{test})|_{\\Theta=U(\\theta, D^{tr})}}_{(1)} \\underbrace{\\dfrac{d}{d \\theta} U(\\theta, D^{tr})}_{(2)}. && (\\text{via chain rule})\n",
        "\\end{align*}\n",
        "\n",
        "(1) is a row vector which can be computed through a single backwards pass of the network, when setting parameters to $\\Theta$ then differentiating loss $\\mathcal{L}$ with respects to $\\Theta$.\\\\ The hessian matrix (2) is obtained through differentiating the update rule $U(\\theta, D^{tr}) = \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\theta, D^{tr})$ with respects to $\\theta$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99HhXEPbON_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MAML():\n",
        "  def __init__(self, model, tasks, inner_lr, meta_lr, K=10, inner_steps=1, tasks_per_meta_batch=1000, criterion=nn.MSELoss(reduction='mean')):\n",
        "    self.tasks = tasks # Instance of Sine_Task_Distribution\n",
        "    self.model = model # Meta-model\n",
        "    self.criterion = criterion # Usually MSE\n",
        "    self.meta_optimizer = torch.optim.SGD(model.parameters(), meta_lr) # Meta-optimiser for outer loop updates\n",
        "\n",
        "    self.inner_lr = inner_lr\n",
        "    self.meta_lr = meta_lr\n",
        "    self.K = K \n",
        "    self.inner_steps = inner_steps\n",
        "    self.tasks_per_meta_batch = tasks_per_meta_batch\n",
        "\n",
        "  def inner_loop(self, task):\n",
        "    \"\"\"\n",
        "        Computes inner-loop optimisation:\n",
        "        PARAMETERS:\n",
        "        1. task - takes an instantiation of tasks distribution\n",
        "\n",
        "        ALGORITHM:\n",
        "        1. Evaluate gradient of the loss of the task with respects to meta-parameters over K-examples of task\n",
        "        2. Compute adapted parameters with gradient descent\n",
        "      \"\"\"\n",
        "    # Sample K training data from this task for inner-loop training\n",
        "    X_train, y_train = task.sample_data(self.K)\n",
        "\n",
        "    # Store copy of model\n",
        "    adapted_model = copy.deepcopy(self.model)\n",
        "    for param in self.model.parameters():\n",
        "      param.requires_grad = False\n",
        "    for param in adapted_model.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "    # Initialise inner optimiser\n",
        "    inner_opt = torch.optim.SGD(adapted_model.parameters(), self.inner_lr)\n",
        "\n",
        "    # Perform inner_steps gradient steps (usually 1 to avoid nasty mathematics)\n",
        "    for _ in range(self.inner_steps):\n",
        "      \n",
        "      # Zero gradients in case of multiple inner-loop gradient steps\n",
        "      inner_opt.zero_grad()\n",
        "\n",
        "      # Forward pass using meta-parameters\n",
        "      y_pred = adapted_model(X_train)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = self.criterion(y_pred, y_train)\n",
        "\n",
        "      # Compute adapted parameters in adapted_model\n",
        "      # Pass create_graph=True to instruct model to keep original computation\n",
        "      # graph around to allow for second-order derivatives to be calculated\n",
        "      # for meta-training in the outer-loop.\n",
        "      loss.backward(retain_graph=True, create_graph=True)\n",
        "      inner_opt.step()\n",
        "      \n",
        "    # Now we have adapted parameters:\n",
        "    # Compute loss using new data, sampled from the same task using\n",
        "    # adapted parameters\n",
        "    X_train, y_train = task.sample_data(self.K)\n",
        "    y_pred = adapted_model(X_train)\n",
        "    loss = self.criterion(y_pred, y_train)\n",
        "\n",
        "    return loss\n",
        "    \n",
        "  def outer_loop(self, num_iterations):\n",
        "    # use self.meta_optimizer so that every loop updates it for inner_loop function\n",
        "    \"\"\"\n",
        "      Computes outer-loop optimisation:\n",
        "      PARAMETERS:\n",
        "      1. num_iterations - number of outer-loop gradient descent steps\n",
        "\n",
        "      ALGORITHM:\n",
        "      1. Sample batch of tasks for inner-loop optimisation\n",
        "      2. Compute inner-loop\n",
        "      3. Update meta-parameters using gradient descent\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for iteration in range(0, num_iterations):\n",
        "      \n",
        "\n",
        "      meta_loss = 0\n",
        "      for i in range(self.tasks_per_meta_batch):\n",
        "        # Generate task from task distribution\n",
        "        task = self.tasks.sample_task()\n",
        "        # Compute task specific loss with adapted gradients\n",
        "        meta_loss += self.inner_loop(task) # do not need to add meta_loss ? should go in inner loop\n",
        "      make_dot(meta_loss).view()\n",
        "\n",
        "      # Compute meta gradient of \"meta-loss\" w.r.t. meta-parameters\n",
        "      for param in self.model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "      self.meta_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "      meta_loss.backward(retain_graph=True) # not computing gradients wrt meta parameters because they aren't in the graph\n",
        "      #self.meta_optimizer.step()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfmzx937OHyb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f363d630-052b-4052-9d8f-bb44f2921fe3"
      },
      "source": [
        "torch.manual_seed(2)\n",
        "model1 = Regressor().cuda()\n",
        "a = Sine_Task_Distribution(0.1,5,0,np.pi,-5,5)\n",
        "task = a.sample_task()\n",
        "print(task.xmin)\n",
        "print(task.xmax)\n",
        "maml = MAML(model1, a, 0.01, 0.01, 10, 1 , 3)\n",
        "for p in model1.parameters(): print(p,p.grad)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-5\n",
            "5\n",
            "Parameter containing:\n",
            "tensor([[ 0.2294],\n",
            "        [-0.2380],\n",
            "        [ 0.2742],\n",
            "        [-0.0511],\n",
            "        [ 0.4272],\n",
            "        [ 0.2381],\n",
            "        [-0.1149],\n",
            "        [-0.8085],\n",
            "        [ 0.2283],\n",
            "        [-0.8853],\n",
            "        [ 0.1314],\n",
            "        [ 0.0665],\n",
            "        [-0.2199],\n",
            "        [ 0.8177],\n",
            "        [ 0.0667],\n",
            "        [ 0.4147],\n",
            "        [ 0.4232],\n",
            "        [-0.5899],\n",
            "        [-0.3844],\n",
            "        [ 0.9617],\n",
            "        [-0.9795],\n",
            "        [-0.0679],\n",
            "        [-0.0792],\n",
            "        [ 0.7093],\n",
            "        [-0.0951],\n",
            "        [ 0.2633],\n",
            "        [-0.0480],\n",
            "        [-0.5599],\n",
            "        [-0.5668],\n",
            "        [-0.4858],\n",
            "        [-0.9084],\n",
            "        [-0.6490],\n",
            "        [ 0.2353],\n",
            "        [ 0.6581],\n",
            "        [ 0.0493],\n",
            "        [-0.4584],\n",
            "        [ 0.4395],\n",
            "        [-0.3839],\n",
            "        [-0.2215],\n",
            "        [-0.5482]], device='cuda:0', requires_grad=True) None\n",
            "Parameter containing:\n",
            "tensor([-0.3140, -0.9266,  0.4267,  0.3888,  0.1986,  0.4910,  0.4238,  0.0442,\n",
            "         0.1059,  0.0764,  0.5336,  0.6717,  0.7181,  0.5796, -0.2438, -0.0445,\n",
            "        -0.2032,  0.5817,  0.1111,  0.9255,  0.5072, -0.8547,  0.2925,  0.9609,\n",
            "         0.8882, -0.0157,  0.3318, -0.9381, -0.3188,  0.4876, -0.9110,  0.8712,\n",
            "        -0.6576,  0.3162, -0.0379,  0.1762,  0.0969, -0.9348, -0.2148, -0.6321],\n",
            "       device='cuda:0', requires_grad=True) None\n",
            "Parameter containing:\n",
            "tensor([[ 0.1344, -0.0194, -0.1574,  ...,  0.1409, -0.0099,  0.0857],\n",
            "        [ 0.1158,  0.0704, -0.1375,  ...,  0.0035,  0.1466,  0.0566],\n",
            "        [-0.1052,  0.1091,  0.0130,  ..., -0.0857,  0.0642,  0.1209],\n",
            "        ...,\n",
            "        [-0.1085, -0.1577, -0.1253,  ...,  0.0924, -0.1414, -0.0675],\n",
            "        [-0.0976, -0.0287, -0.1030,  ..., -0.0633,  0.1438,  0.0951],\n",
            "        [-0.0925, -0.1294,  0.0092,  ..., -0.0031,  0.0278, -0.0240]],\n",
            "       device='cuda:0', requires_grad=True) None\n",
            "Parameter containing:\n",
            "tensor([-0.0184, -0.0007,  0.1014, -0.0213,  0.0988,  0.0435,  0.0873, -0.1086,\n",
            "         0.1175,  0.0710,  0.0604, -0.1220,  0.1164,  0.0072,  0.0358,  0.1451,\n",
            "        -0.1285, -0.0881, -0.1315,  0.1240,  0.0500, -0.1087,  0.0425,  0.1564,\n",
            "         0.0679,  0.1024,  0.0827, -0.1301, -0.1229,  0.0124,  0.0207,  0.0373,\n",
            "        -0.1517, -0.0611, -0.0193,  0.0638, -0.1276, -0.1503, -0.0698, -0.0208],\n",
            "       device='cuda:0', requires_grad=True) None\n",
            "Parameter containing:\n",
            "tensor([[-0.0844, -0.0387,  0.0473,  0.0165, -0.1160, -0.0429, -0.0329, -0.1460,\n",
            "         -0.0095, -0.1091, -0.1074, -0.0452, -0.0052,  0.1381, -0.0390,  0.1143,\n",
            "         -0.0024,  0.0869, -0.0776, -0.0679, -0.1296,  0.0105, -0.0343,  0.0237,\n",
            "          0.1218, -0.1360,  0.1519, -0.0064,  0.0051,  0.0140, -0.0990, -0.1309,\n",
            "         -0.1324,  0.1216, -0.0309,  0.0387, -0.0370,  0.1187, -0.1118, -0.0244]],\n",
            "       device='cuda:0', requires_grad=True) None\n",
            "Parameter containing:\n",
            "tensor([0.1336], device='cuda:0', requires_grad=True) None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}